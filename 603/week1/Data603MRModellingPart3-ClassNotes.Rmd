---
title: "Data 603:Statistical Modelling with Data"
output:
  html_notebook: default
  word_document: default
---
<style type="text/css">

body{ /* Normal  */
      font-size: 18px;
  }
td {  /* Table  */
  font-size: 8px;
}
.title {
  font-size: 38px;
  color: DarkRed;
}
 p {line-height: 2em;}
h1 { /* Header 1 */
  font-size: 28px;
  color: DarkBlue;
}
h2 { /* Header 2 */
    font-size: 22px;
  color: DarkBlue;
}
h3 { /* Header 3 */
  font-size: 18px;
  font-family: "Times New Roman", Times, serif;
  color: DarkBlue;
}
code.r{ /* Code block */
    font-size: 12px;
}
pre { /* Code block - determines code spacing between lines */
    font-size: 14px;
}
</style>

## Multiple Linear Regression 
## Part III: Model Selection 

&copy; Thuntida Ngamkham 2022

## Model Selection

One of the biggest problems in building a model to describe a response variable ($Y$) is choosing the important independent variables to be included. The list of potentially important independent variables is extremely long and we need some objective methods of screening out those which are not important. The problem of deciding which of a large set of independent variables to include in a model is a common one. 

__For example: Independent Variables in the Executive Salary__

Independent Variable and Description

x~1~: Experience (years)-quantitative

x~2~: Education (years)-quantitative

x~3~: Bonus eligibility (1 if yes, 0 if no)-qualitative

x~4~: Number of employees supervised-quantitative

x~5~: Corporate assets (millions of dollars)-quantitative

x~6~: Board member (1 if yes, 0 if no)-qualitative

x~7~: Age (years)-quantitative

x~8~: Company profits (past 12 months, millions of dollars)-quantitative

x~9~: Has international responsibility (1 if yes, 0 if no)-qualitative

x~10~: Company's total sales (past 12 months, millions of dollars)-quantitative


## Steps in Selecting the Best Regression Equation

To select the best regression equation, carry out the following steps

1. Specify the maximum model to be considered.

2. Specify a strategy for selecting a model

3. Evaluate the reliability of the model chosen.

By following theses steps, you can convert the fuzzy idea of finding the best predictors of $Y$ into simple, concrete action. Each step helps to ensure reliability and to reduce the work required.

## Step 1: Specifying the Maximum Model

The maximum model is defined to be the largest model (the one having the most predictor variables) considered at any point in the process of model selection. A model created by deleting predictors from the maximum model is called _a restriction of the maximum model_.



## Step 2: Specify a strategy for selecting a model 
A systematic approach to building a restriction model from a large number of independent variables is difficult because the interpretation of multivariable interactions is complicated. We therefore turn to a screening procedure, available in most statistical software packages, objectively determine which independent variables in the list are the most important predictors of $Y$ and which are the least important predictors. The most widely used method is **stepwise regression**, while another popular method, **backward** and **forward regression**, also are provided in this section.


## Stepwise Regression Procedure


The user first identifies the response $y$ and the set of potentially important independent variables $x_1$, $x_2$,...,$x_p$, where $p$ is generally large. However, we often __include only the main effects__ of both quantitative variables (first-order terms) and qualitative variables (dummy variables). The response and independent variables are then entered into the computer software, and the stepwise procedure begins.

__Step 1__ The software program fits all possible one-variable models of the form
$$
\begin{aligned}
E(Y)=\beta_0+\beta_1X_i
\end{aligned}
$$
to the data, where $X_i$ is the $i$th independent variable, $i = 1,2,...,p$. For each model, 
the t-test for a single $\beta_1$ parameter is conducted to test the null hypothesis

H~0~: $\beta_1$ = 0

against the alternative hypothesis

H~a~: $\beta_1\neq0$


The independent variable that produces the largest (absolute) t -value is then
declared the best one-variable predictor of $Y$. Call this independent variable $X_1$.

```{r,warning=FALSE}
library(olsrr)#need to install the package olsrr
salary=read.csv("c:/Users/thunt/OneDrive - University of Calgary/dataset603/EXECSAL2.csv", header = TRUE)
model1<-lm(Y~X1, data = salary)
summary(model1)
model2<-lm(Y~X2, data = salary)
summary(model2)
model3<-lm(Y~X3, data = salary)
summary(model3)
model4<-lm(Y~X4, data = salary)
summary(model4)
model5<-lm(Y~X5, data = salary)
summary(model5)
model6<-lm(Y~X6, data = salary)
summary(model6)
model7<-lm(Y~X7, data = salary)
summary(model7)
model8<-lm(Y~X8, data = salary)
summary(model8)
model9<-lm(Y~X9, data = salary)
summary(model9)
model10<-lm(Y~X10, data = salary)
summary(model10)

```
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
__Step 2__ The stepwise program now begins to search through the remaining $(p - 1)$ independent variables for the best two-variable model of the form

$\hat{Y}=\beta_0+\beta_1X_1+\beta_2X_i$

This is done by fitting all two-variable models containing $X_1$ and each of the other $(p - 1)$ options for the second variable $X_i$. The t-values for the test $H_0 : \beta_2 = 0$ are computed for each of the $p-1$ models (corresponding to the remaining independent variables,
$X_i, i = 2, 3, . . . , p-1$), and the variable having the largest $t$ is retained. Call this variable $X_2$.

Before proceeding to Step 3, the stepwise routine will go back and check the t-value of $\hat{\beta_1}$ after $\hat{\beta_2}X_2$ has been added to the model. If the t-value has become nonsignificant at some specified $\alpha$ level (say $\alpha=0.3$), the variable $X_1$ is removed and a search is made for the independent variable with a $\beta$ parameter that will yield the most significant t-value in the presence of $\hat{\beta_2}X_2$. 

The reason the t-value for $X_1$ may change from step 1 to step 2 is that the meaning of the coefficient $\hat{\beta_1}$ changes. In step 2, we are approximating a complex response surface in two variables with a plane. The best-fitting plane may yield a different value for $\hat{\beta_1}$ than that obtained in step 1. Thus, both the value of $\hat{\beta_1}$ and its significance usually changes from step 1 to step 2. For this reason, stepwise procedures that recheck the t-values at each step are preferred.


```{r,warning=FALSE}
library(olsrr)#need to install the package olsrr
salary=read.csv("c:/Users/thunt/OneDrive - University of Calgary/dataset603/EXECSAL2.csv", header = TRUE)
model1<-lm(Y~X1+X2, data = salary)
summary(model1)
model2<-lm(Y~X1+X3, data = salary)
summary(model2)
model3<-lm(Y~X1+X4, data = salary)
summary(model3)
model4<-lm(Y~X1+X5, data = salary)
summary(model4)
model5<-lm(Y~X1+X6, data = salary)
summary(model5)
model6<-lm(Y~X1+X7, data = salary)
summary(model6)
model7<-lm(Y~X1+X8, data = salary)
summary(model7)
model8<-lm(Y~X1+X9, data = salary)
summary(model8)
model9<-lm(Y~X1+X10, data = salary)
summary(model9)
```
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

__Step 3__ The stepwise regression procedure now checks for a third independent variable to include in the model with $X_1$ and $X_2$. That is, we seek the best model of the form

$E(Y) = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_i$

To do this, the computer fits all the $(p-2)$ models using $X_1,X_2,$ and each of the $(p-2)$ remaining variables, $X_i$ , as a possible $X_3$. The criterion is again to include the independent variable with the largest (significant) t-value. Call this best third variable $X_3$. The better programs now recheck the t-values corresponding to the $X_1$ and $X_2$ coefficients, replacing the variables that yield nonsignificant t-values. 

This procedure is continued until no further independent variables can be found that yield significant t-values (at the specified $\alpha$ level) in the presence of the variables already in the model.

<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

Refer to the Executive Salary Example. A preliminary step in the construction of this model is the determination of the most important independent variables. For one firm, 10 potential independent variables (seven quantitative and three qualitative) were measured in a sample of 100 executives. The data are saved in the __EXECSAL2.CSV__ file. Since it would be very difficult to construct a complete first-order model with all of the 10 independent variables, use stepwise regression to decide which of the 10 variables should be included in the building of the final model.



```{r,warning=FALSE}
library(olsrr)#need to install the package olsrr
salary=read.csv("c:/Users/thunt/OneDrive - University of Calgary/dataset603/EXECSAL2.csv", header = TRUE)
fullmodel<-lm(Y~X1+X2+factor(X3)+X4+X5+factor(X6)+X7+X8+factor(X9)+X10, data = salary)
summary(fullmodel)
stepmod=ols_step_both_p(fullmodel,pent = 0.1, prem = 0.3, details=TRUE)
summary(stepmod$model)
```



*R functions*
*ols_step_both_p(): Build regression model from a set of candidate predictor variables by entering and removing predictors based on p values*

*Note!*

*pent: variables with p value less than pent will enter into the model.*

*prem: variables with p value more than prem will be removed from the model.*

*details: print the regression result at each step.*


From the output, the regression model is $Y=X_1+X_2+X_3+X_4+X_5+\epsilon$. Is this model the best fit for predicting executive salary?
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

## Inclass Practice Problem 10

From the credit example in MLR Modelling Part 2, use __Stepwise Regression Procedure__ to find the potentially important independent variables for predicting credit card balance.

<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
## Backward Elimination Procedure

The Backward procedure initially fits a model containing terms for all potential independent variables. That is, for $p$ independent variables, the model $E(Y) = \beta_0 + \beta_1X_1 + \beta_2X_2 +...+\beta_pX_p$ is fit in step 1. The variable with the smallest t (or F) statistic for testing $H_0 :\beta_i = 0$ is identified and dropped from the model if the t-value is less than some specified critical value or p-value more than a cut-off. The model with the remaining $(p-1)$ independent variables is fit in step 2, and again, the variable associated with the smallest nonsignificant t-value is dropped. This process is repeated until no further nonsignificant independent variables can be found.


```{r}
library(olsrr) #need to install the package olsrr
salary=read.csv("c:/Users/thunt/OneDrive - University of Calgary/dataset603/EXECSAL2.csv", header = TRUE)
fullmodel<-lm(Y~X1+X2+factor(X3)+X4+X5+factor(X6)+X7+X8+factor(X9)+X10, data = salary)
backmodel=ols_step_backward_p(fullmodel, prem = 0.3, details=TRUE)
summary(backmodel$model)
```
*R functions*
*ols_step_backward_p():Build regression model from a set of candidate predictor variables by removing predictors based on p values*

From the output, the first order regression model by using Backward Regression Procedure is $Y=\beta_0+\beta_1X_1+\beta_2X_2+\beta_2X_3+\beta_3X_4+\beta_4X_5+\beta_5X_9+\epsilon$. Consider the predictor X9 has tcal=-1.287 with the p-value= 0.201, this predictor should be dropped out from the output. Therefore, $Y=\beta_0+\beta_1X_1+\beta_2X_2+\beta_2X_3+\beta_3X_4+\beta_4X_5+\epsilon$ is the first order model to predict salary by using For Backward Regression Procedure.


## Inclass Practice Problem 11

From the credit example in MLR Modelling Part 2, use __Backward Regression Procedure__ to find the potentially important independent variables for predicting credit card balance.

<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

## Forward selection procedure

This method is nearly identical to the stepwise procedure previously outlined. The only difference is that the forward selection technique provides no option for rechecking the t-values corresponding to the $X$'s that have entered the model in an earlier step.


```{r}
library(olsrr) #need to install the package olsrr
salary=read.csv("c:/Users/thunt/OneDrive - University of Calgary/dataset603/EXECSAL2.csv", header = TRUE)
fullmodel<-lm(Y~X1+X2+factor(X3)+X4+X5+factor(X6)+X7+X8+factor(X9)+X10, data = salary)
formodel=ols_step_forward_p(fullmodel,penter = 0.1, details=TRUE)
summary(formodel$model)
```

*R functions*
*ols_step_forward_p():Build regression model from a set of candidate predictor variables by entering predictors based on p values*
*penter:	p value; variables with p value less than penter will enter into the model. By default, penter=0.3*

From the output, we specified our penter = 0.1 to follow the same procedure of Stepwise regression. Therefore, the regression model by using Forward Regression Procedure is 

$$
Y=\beta_0+\beta_1X_1+\beta_2X_2+\beta_3X_3+\beta_4X_4+\beta_5X_5+\epsilon. 
$$

## Inclass Practice Problem 12

From the credit example in MLR Modelling Part 2, use __Forward Regression Procedure__ to find the potentially important independent variables for predicting credit card balance.

<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

__Note!__

R also provides a function for selecting a subset of predictors from a larger set. You can use stepwise selection (backward,forward,both) by using the stepAIC() function from the MASS package. This function will select variable by extracting AIC (AIC value is explained in the next topic).

__CAUTION__ Be aware of using the results of stepwise regression to make inferences about the relationship between $E(Y)$ and the independent variables in the first order model. 

__First__, an extremely large number of t-tests have been conducted, leading to a high probability of making more Type I errors.

__Second__, stepwise regression should be used only when necessary- that is when you want to determine which of a large number of potentially important independent variables should be used in the model building process.

## All-Possible-Regressions Selection Procedure

We presented stepwise regression as an objective screening procedure. Stepwise does not only provide the largest t-value, but also the techniques differ with respect to the criteria for selecting the ''best'' subset of variables. In this section, we describe four criteria widely used in practice, 

__1. $R^2$ Criterion__
the multiple coefficient of determination

$$
\begin{aligned}
R^2=\frac{SSR}{SST}=1-\frac{SSE}{SST}
\end{aligned}
$$

will increase when independent variables are added to the model. Therefore, the model that includes all $p$ independent variables $E(Y) =\beta_0 +\beta_1X_1 + \beta_2X_2 +...+\beta_pX_p$ will yield the largest $R^2$. 

<!--In practice, the best model found by the $R^2$ criterion will rarely be the model with the largest $R^2$. Generally, you are looking for a simple model that is as good as, or nearly as good as, the model with all $p$ independent variables. But unlike that in stepwise regression, the decision about when to stop adding variables to the model is a subjective one. -->

__2. Adjusted $R^2$ or RMSE Criterion__

We can use the adjusted $R^2$ instead of $R^2$. It is easy to show that $R^2_{adj}$ is related to MSE as follows:
$$
\begin{aligned}
R^2_{adj}&=1-\frac{\frac{SSE}{n-p-1}}{\frac{SST}{n-1}}\\
R^2_{adj}&=1-(n-1)\frac{MSE}{SST}\\
s&=RMSE=\sqrt{\frac{1}{n-p-1}SSE}
\end{aligned}
$$
Note that $R^2_{adj}$ increases only if RMSE decreases [since SST remains constant for all models]. Thus, an equivalent procedure is to search for the model with the minimum, or near minimum, RMSE.

__3. Mallows's Cp Criterion__

The Cp criterion, named for Colin Lingwood Mallow, selects as the best subset model with 

(1) a small value of Cp (i.e., a small total mean square error), means that the model is relatively precise. 

(2) a value of Cp near p + 1, a property that indicates that slight (or no) bias exists in the subset regression model.

Thus, the Cp criterion focuses on minimizing total mean square error and the regression bias. If we are mainly concerned with minimizing total mean square error, we will want to choose the model with the smallest Cp value, as long as the bias is not large. On the other hand, we may prefer a model that yields a Cp value slightly larger than the minimum but that has slight (or no) bias.


__4. AIC (Akaike's information criterion)__

When using the model to predict $Y$, some information will be lost. Akaike's information criterion estimates the relative information lost by a given model. It is defined as

$$
\begin{aligned}
AIC=n\ln(\frac{SSE}{n})+2p
\end{aligned}
$$

The formula is formulated by the statistician __Hirotugu Akaike__. Models with smaller values of AIC are preferred.

*Where*

*n : the number of observations in the dataset*

*p : the number of parameters in the model*


__5. BIC (Bayesian information criteria)__

Bayesian information criterion (BIC) is another criterion for model selection. It is based, in part, on the likelihood function, and it is closely related to Akaike information criterion (AIC). The models can be tested using corresponding BIC values. Lower BIC value indicates a better model.^2^

$$
\begin{aligned}
BIC=n\ln(\frac{SSE}{n})+(p)\ln(n)
\end{aligned}
$$

*Where*

*n : the number of observations in the dataset*

*p : the number of parameters in the model*

In this class, we are going to use R software package to calculate all values.

```{r,warning=FALSE}
# Option 1
library(olsrr)
salary=read.csv("c:/Users/thunt/OneDrive - University of Calgary/dataset603/EXECSAL2.csv", header = TRUE)
firstordermodel<-lm(Y~X1+X2+X3+X4+X5+X6+X7+X8+X9+X10, data= salary)
#Select the subset of predictors that do the best at meeting some well-defined objective criterion, such as having the largest R2 value or the smallest MSE, Mallow's Cp or AIC.
ks=ols_step_best_subset(firstordermodel, details=TRUE)
# for the output interpretation
rsquare<-c(ks$rsquare)
AdjustedR<-c(ks$adjr)
cp<-c(ks$cp)
AIC<-c(ks$aic)
cbind(rsquare,AdjustedR,cp,AIC)
par(mfrow=c(2,2)) # split the plotting panel into a 2 x 2 grid
plot(ks$cp,type = "o",pch=10, xlab="Number of Variables",ylab= "Cp")
plot(ks$rsq,type = "o",pch=10, xlab="Number of Variables",ylab= "R^2")
plot(ks$aic,type = "o",pch=10, xlab="Number of Variables",ylab= "AIC")
plot(ks$adjr,type = "o",pch=10, xlab="Number of Variables",ylab= "Adjusted R^2")
```


*R functions*

*ols_step_best_subset: Best subsets regression, select the subset of predictors that do the best at meeting some well-defined objective criterion, such as having the largest $adj R^2$ value or the smallest MSE, Mallow's Cp or AIC.^1^ BIC values are not provided*


```{r,warning=TRUE}
#  Option 2
library(olsrr)
salary=read.csv("c:/Users/thunt/OneDrive - University of Calgary/dataset603/EXECSAL2.csv", header = TRUE)
firstordermodel<-lm(Y~X1+X2+X3+X4+X5+X6+X7+X8+X9+X10, data= salary)

library(leaps) #need to install the package leaps for regsubsets() function
best.subset<-regsubsets(Y~X1+X2+X3+X4+X5+X6+X7+X8+X9+X10, data= salary, nv=10 ) 
#by default, regsubsets() only reports results up to the best 8-variable model
#Model selection by exhaustive search, forward or backward stepwise, or sequential replacement
#The summary() command outputs the best set of variables for each model size using RMSE.
summary(best.subset)
reg.summary<-summary(best.subset)

# for the output interpretation
rsquare<-c(reg.summary$rsq)
cp<-c(reg.summary$cp)
AdjustedR<-c(reg.summary$adjr2)
RMSE<-c(reg.summary$rss)
BIC<-c(reg.summary$bic)
cbind(rsquare,cp,BIC,RMSE,AdjustedR)
par(mfrow=c(3,2)) # split the plotting panel into a 3 x 2 grid
plot(reg.summary$cp,type = "o",pch=10, xlab="Number of Variables",ylab= "Cp")
plot(reg.summary$bic,type = "o",pch=10, xlab="Number of Variables",ylab= "BIC")
plot(reg.summary$rsq,type = "o",pch=10, xlab="Number of Variables",ylab= "R^2")
plot(reg.summary$rss,type = "o",pch=10, xlab="Number of Variables",ylab= "RMSE")
plot(reg.summary$adjr2,type = "o",pch=10, xlab="Number of Variables",ylab= "Adjusted R^2")
```

*R functions*

*regsubsets() :performs best sub- set selection by identifying the best model that contains a given number of predictors. No AIC values are provided *

-------------------------------------------------------

From the output, the first order regression model is $Y=\beta_0+\beta_1X_1+\beta_2X_2+\beta_3X_3+\beta_4X_4+\beta_5X_5+\epsilon$. Is this model the best fitted model for predicting executive salary?
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

## Inclass practice Problem 13

From the credit card example, using __All Possible Regressions Selection Procedure__ to analyse which independent predictors should be used in the model.

<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

## 3. Evaluate the reliability of the model chosen.

After using model selection by automatic methods or all possible regression methods, we might not have the best fit model yet, as we consider only main effects on independent variables. After eliminating some variables that are not important out of the model, we consider interaction terms and/or high order multiple regression model to improve the model.

```{r}
salary=read.csv("c:/Users/thunt/OneDrive - University of Calgary/dataset603/EXECSAL2.csv", header = TRUE )

firstordermodel<-lm(Y~X1+X2+factor(X3)+X4+X5,data=salary)
summary(firstordermodel)

# building the best model with interation term
interacmodel<-lm(Y~(X1+X2+factor(X3)+X4+X5)^2,data = salary)
summary(interacmodel)

bestinteracmodel<-lm(Y~X1+X2+factor(X3)+X4+X5+factor(X3)*X4,data=salary) 
summary(bestinteracmodel)

#considering high order model between Xs and Y to improve the model
library(GGally) # need to install the GGally package for ggpairs function
#option 1: using function ggpairs()
salarydata <-data.frame(salary$Y,salary$X1,salary$X2,salary$X3,salary$X4,salary$X5)
salarydata
#ggpairs(salarydata)
#LOESS or LOWESS: LOcally WEighted Scatter-plot Smoother
#ggpairs(salarydata,lower = list(continuous = "smooth_loess", combo =
 # "facethist", discrete = "facetbar", na = "na"))
#option2: using function pairs()
#pairs(~Y+X1+X2+factor(X3)+X4+X5,data=salary,panel = panel.smooth) 

bestmodel<-lm(Y~X1+I(X1^2)+X2+factor(X3)+X4+X5+factor(X3)*X4,data=salary)
summary(bestmodel)
bestmodel1<-lm(Y~X1+I(X1^2)+I(X1^3)+X2+factor(X3)+X4+X5+factor(X3)*X4,data=salary)
summary(bestmodel1)
```

*R Functions*
*ggpairs(): look at all pairwise combinations of continuous variables in scatterplots.*
*pairs(): optional function for pairwise combinations*
*panel.smooth: add a smooth loess curve on the scatters*

From the output, you can see that after including an interaction term ($X_3*X_4$) and quadratic term $X^2_1$, they led to such a big improvement in the model as following,

1. all the p-values < 0.05, which means that all regression coefficients were significantly non-zero.

2. $R^2_{adj}$ increases from 0.9164 to 0.9355 

3. Standard error of residuals (RMSE) decreases from 0.07512 to 0.06596

Therefore, it is clear that adding the additional terms really has led to a better fit to the data.
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
## Inclass practice Problem 14

From the credit card example, when we investigate the scatter plots for all pairwise combinations between variables, find the best fitted model to predict balance. You may include interaction terms and higher order terms to improve the model.

<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
## Inclass Practice Problem 15
 
 Clerical staff work hours. In any production process in which one or more workers are engaged in a variety of tasks, the total time spent in production varies as a function of the size of the work pool and the level of output of the various activities. 
 
For example, in a large metropolitan department store, the number of hours worked (Y) per day by the clerical staff may depend on the following 
 
variables: 

X1 = Number of pieces of mail processed (open, sort, etc.)

X2 = Number of money orders and gift certificates sold,

X3 = Number of window payments (customer charge accounts) transacted ,

X4 = Number of change order transactions processed ,

X5 = Number of checks cashed ,

X6 =Number of pieces of miscellaneous mail processed on an ''as available'' basis , and

X7 =Number of bus tickets sold 


The data are provided in __CLERICAL.csv__ file count for these activities on each of 52 working days. Conduct a Stepwise Regression Procedure and All-Possible-Regressions procedure of the data using R software package. 


<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>

^1^https://www.rdocumentation.org/packages/olsrr/versions/0.5.3/topics/ols_step_best_subset

^2^https://medium.com/@analyttica/what-is-bayesian-information-criterion-bic-b3396a894be6

## References

_-Gareth James & Daniela Witten & Trevor Hastie Robert Tibshirani, An Introduction to Statistical Learning with Applications in R: Springer New York Heidelberg Dordrecht London._

_-Wickham and Grolemund, R for Data Science: O'Reilly Media_

